---
title: "St. Michael's Hospital General Internal Medicine Dataset (2)"
subtitle: "Health Data Nexus Workshop"
date: "April 25, 2025"
author: "Chloe Pou-Prom"
format: 
  revealjs:
    theme: [default, custom.scss]
    embed-resources: true
brand: _brand.yml
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, echo = TRUE, warning = FALSE)
# Note: This is the file that I ran and rendered on my local environment
# It will not work on the HDN environment, however I'm including it
# so that you can see the exact code I used to generate the rendered output.
```

## 

In the previous slides, we explored the raw unprocessed tables. In these slides, we will briefly go over the available pre-processed tables, then we will work towards building a simple model. In order to do so, we will go through the steps required to process the data:

::: {.incremental}
- Process the outcomes data

- Create a patient timeline

- Process the vitals data

- Build a model to predict deterioration from vitals
:::

## Data processing 

The dataset is provided in its original raw form as well as in a pre-processed form which aggregates data into fixed time windows. Pre-processing is done as follows:

::: {.incremental}
- **Time-varying data is binned into 8 hour windows**

- **Numeric data is averaged within each window, trimmed, and normalized**. Two variables are added: an indicator for measurement, and a time since last measurement

- **Missing numeric data is carried forward with mean imputation**
:::

## Data processing {.incremental}

The dataset is provided in its original, raw form as well as in a pre-processed form which aggregates data into fixed time windows. Pre-processing is done as follows:

::: {.incremental}
- Orders are given as indicator variable

- Missing orders are imputed as zero

- Medications are grouped into classes and then classes are given as indicator variables
:::

## Let's process some data!

Load in the relevant packages:

```{r}
library(dplyr) # for data processing
library(ggplot2) # for plotting
library(knitr) # for rendering nice tables
library(DT)
```

Since we will be sampling data, we set the seed to ensure reproducibility:

```{r}
set.seed(123)
```

## Let's process some data!

```{r}
#| code-line-numbers: "|4-13"
DATA_DIR <- "/mnt/research/LKS-CHART/administration/00_archive/external_applications/datathon/data/"
PROCESSED_DATA_DIR <- file.path(DATA_DIR, "pre-processed")
RAW_DATA_DIR <- file.path(DATA_DIR, "raw-data")
encounters <- rbind(
  # read.csv(file.path(RAW_DATA_DIR, "train", "train_encounters.csv")),
  # read.csv(file.path(RAW_DATA_DIR, "valid", "valid_encounters.csv")),
  read.csv(file.path(RAW_DATA_DIR, "test", "test_encounters.csv"))
)
measurements <- rbind(
  # read.csv(file.path(RAW_DATA_DIR, "train", "train_numeric_variables.csv")),
  # read.csv(file.path(RAW_DATA_DIR, "valid", "valid_numeric_variables.csv")),
  read.csv(file.path(RAW_DATA_DIR, "test", "test_numeric_variables.csv"))
)

```

## Outcomes timeseries

The outcomes data is provided as a time-varying table:

```{r}
#| output-location: slide
outcomes <- rbind(
  # read.csv(file.path(PROCESSED_DATA_DIR, "train", "train_outcome_timeseries_8hr.csv")),
  # read.csv(file.path(PROCESSED_DATA_DIR, "valid", "valid_outcome_timeseries_8hr.csv")),
  read.csv(file.path(PROCESSED_DATA_DIR, "test", "test_outcome_timeseries_8hr.csv"))
)

outcomes |> 
  head(100) |>
  datatable(options = list(pageLength = 4))
```

::: {.notes}
Notice: for every encounter, there are rows for window and outcome_x_hours columns
:::

## Outcomes timeseries


How did we get here?

::: {.incremental}
- Time-varying data is binned into 8-hour windows starting from the time the patient enters the GIM ward. The timeline ends when the patient experiences one of the outcomes.

- Outcome is defined as the first occurrence of any of the following events (i.e. if a patient transfers to the ICU and then dies, their outcome will be ICU transfer). 
:::

## Creating a patient timeline

Let's start with an example on 1 patient

```{r}
#| output-location: slide
example_enc <- encounters |> 
  filter(OUTCOME_ALL == 1) |>
  select(ENCOUNTER_NUM, OUTCOME_ALL, time_to_event) |>
  head(1)
example_enc |> 
  kable()
```

## Creating a patient timeline

::: {.incremental}

- The timeline will have **1 row for every hour the patient is in the hospital**. 

- We later want to group the timeline into **8-hour windows**. 

- In order to do this, I create an indicated variable `window_8hr` which will assign a different value to each 8-hour block.

:::

## Creating a patient timeline {.hdn}


```{r}
#| code-line-numbers: "|1-2|14"
#| output-location: slide

# 1 row for every hour the patient is in the hospital
timeline_hours <- seq(1, ceiling(example_enc$time_to_event))

example_timeline <- tibble(
  ENCOUNTER_NUM = example_enc$ENCOUNTER_NUM,
  hour = seq(1, ceiling(example_enc$time_to_event)),
) |>
  # Group into 8 hour windows
  mutate(
  #   a = hour %% 8, # modulo
  #   b = hour %% 8 == 1,
  #   c = cumsum(hour),
  #   d = cumsum(hour %% 8 == 1),
    window_8hr = cumsum((hour) %% 8 == 1))

example_timeline |> 
  head() |>
  kable()
```

## Creating a patient timeline {.hdn}

We can then start merging events to our timeline.

First, we add the outcome to the timeline:

```{r}
#| output-location: slide
outcome_hour <- ceiling(example_enc$time_to_event)

example_timeline |> 
  mutate(outcome_all = ifelse(hour == outcome_hour, 1, 0)) |>
  head(100) |>
  datatable(options = list(pageLength = 4))
```

## Processing the vitals 

Next, we'll add the vitals.

But first, let's process the vitals a bit.

Let's extract some summary statistics for the vitals:


```{r}
#| code-line-numbers: "|1-7|9-12"
measurements_summary <- measurements |>
  group_by(variable) |>
  summarize(
    mean = mean(numeric_value),
    q01 = quantile(numeric_value, probs = 0.01),
    q99 = quantile(numeric_value, probs = 0.99)
  )

example_vitals <- measurements |>
  filter(variable %in% c("vital_sbpdiastolic", "vital_sbpsystolic", "vital_sfio2", "vital_s02saturation", "vital_spulse", "vital_srespirations", "vital_stemperature")) |>
  filter(ENCOUNTER_NUM  == example_enc$ENCOUNTER_NUM)

```

## Processing the vitals 

::: {.incremental}

- First, we will **trim** the vitals (based on the 1st and 99th quantile).

- Then, we group the vitals at the **hourly level**.

- Then, I can **pivot** my dataframe to a _wide_ format. This will give me one column per vital.
:::

::: {.notes}
Trim - there are other alternatives to dealing with outliers. You can remove them or replace them with the average

Hourly: remember, sometimes the same vital is measured multiple times in a short period of time


:::

## Processing the vitals {.hdn}

Transform the vitals data: instead of one row per event, we want one row per hour

```{r}
#| code-line-numbers: "|1-2|4-11|13-18|20-23"
example_vitals_hour <- example_vitals |>
  left_join(measurements_summary, by = "variable") |>
  
  # Trim measurements
  mutate(
    trimmed_value = case_when(
      numeric_value <= q01 ~ q01,
      numeric_value >= q99 ~ q99,
      TRUE ~ numeric_value
    )
  ) |>
  select(-q01, -q99, -mean, -numeric_value) |>
  
  # Group by the hour and take the average
  mutate(hour = ceiling(time)) |>
  select(-time) |>
  group_by(ENCOUNTER_NUM, variable, hour) |>
  summarize_all(mean) |>
  ungroup() |>
  
  # Pivot from long to wide dataframe
  tidyr::pivot_wider(names_from = variable, values_from = trimmed_value) |>
  arrange(hour)
```

## Processing the vitals 

::: {.incremental}

- We can merge the processed hourly vitals to our hourly timeline.

- To impute missing values, we will use: _last observation carried forward_

- Next, we can group by **8-hour windows**.

- Pivot the dataframe back to a _long_ format and merge it with the summary statistics, in order to **normalize**.

- Pivot the dataframe back a _wide_ format.

:::

::: {.notes}
Dealing with missingness: LOCF works because we are interested in carrying forward in time.

The reason we use 8 hours is because 1) we don't want to end up with a dataframe that is too big for model training and 2) typically, labs/vitals get measured every 6-8 hours.

Normalize: scale the data between 0 and 1 
:::

## Processing the vitals {.hdn}

Hourly vitals data can then be merged to the hourly timeline and we can apply _last observation carried forward_

```{r}
#| code-line-numbers: "|1-2|6-7|10-13|15-18|20-22|24-25"
example_vitals_timeline <- example_timeline |>
  left_join(example_vitals_hour, by = c("ENCOUNTER_NUM", "hour")) 

example_processed_vitals_timeline <- example_vitals_timeline |>
  
  # LOCF
  tidyr::fill(contains('vital_')) |>
  select(-hour) |>
  
  # Group by 8-hour window and take mean
  group_by(ENCOUNTER_NUM, window_8hr) |>
  summarize_all(mean) |>
  ungroup() |>
  
  # Transform back to a long dataframe, so that this can be merged with the summary
  tidyr::pivot_longer(cols = contains("vital_"), names_to = "variable") |>
  left_join(measurements_summary, by = "variable") |>
  mutate(filled_value = ifelse(is.na(value), mean, value)) |>
  
  # Normalize
  mutate(normalized_value = (value - q01) / (q99 - q01)) |>
  select(ENCOUNTER_NUM, window_8hr, variable, normalized_value) |>
  
  # Transform back to a wide dataframe
  tidyr::pivot_wider(names_from = variable, values_from = normalized_value)

```

## Processing the vitals {.hdn .scrollable}

Before processing:

```{r}
#| output-location: slide
example_vitals_timeline |>
  datatable(options = list(pageLength = 4), caption = "Example vitals timeline")
```

## Processing the vitals  {.hdn .scrollable}

After processing:

```{r}
#| output-location: slide
example_processed_vitals_timeline |>
  datatable(options = list(pageLength = 4), caption = "Example processed vitals timeline")
```


## Processing vitals for all patients 

Whew... that was for just one encounter. Let's do it for all encounters!

First, create the timeline

```{r}
#| code-line-numbers: "|2|9-13"
all_encounters_timeline_list <- list()
for (i in 1:nrow(encounters)) {
  encounter <- encounters |>
    slice(i)
  
  # For each encounter, we're going 
  # to create a timeline starting from
  # 1 to the time of the event
  encounter_timeline <- tibble (
    ENCOUNTER_NUM = encounter$ENCOUNTER_NUM,
    hour = seq(1, ceiling(encounter$time_to_event))
  ) |>
    mutate(window_8hr = cumsum((hour ) %% 8 == 1))
  all_encounters_timeline_list[[i]] <- encounter_timeline
}
all_timeline <- do.call(rbind, all_encounters_timeline_list)
```

## Processing vitals for all patients

```{r}
#| output-location: slide
all_timeline |>
  head(200) |>
  datatable(options = list(pageLength = 5))
```

## Processing vitals for all patients {.skip}

This tells us how long each patient's timeline is (i.e., how long the encounter is)

```{r}
#| output-location: slide
all_timeline |>
  count(ENCOUNTER_NUM) |>
  head() |>
  kable()
```

## Processing vitals for all patients

```{r}
all_vitals <- measurements |>
  filter(variable %in% c("vital_sbpdiastolic", "vital_sbpsystolic", "vital_sfio2", "vital_s02saturation", "vital_spulse", "vital_srespirations", "vital_stemperature"))
```

## Processing vitals for all patients 

Summarize at the hourly interval

```{r}
#| code-line-numbers: "|2-9|12|14-16|17"
all_vitals_hour <- all_vitals |>
  left_join(measurements_summary, by = "variable") |>
  mutate(
    numeric_value = case_when(
      numeric_value <= q01 ~ q01,
      numeric_value >= q99 ~ q99,
      TRUE ~ numeric_value
    )
  ) |>
  select(-q01, -q99, -mean)|>
  
  mutate(hour = ceiling(time)) |>
  select(-time) |>
  group_by(ENCOUNTER_NUM, variable, hour) |>
  summarize_all(mean) |>
  ungroup() |>
  tidyr::pivot_wider(names_from = variable, values_from = numeric_value) |>
  arrange(hour)
```

## Processing vitals for all patients 

Join to the hourly timeline

```{r}
all_vitals_timeline <- all_timeline |>
  left_join(all_vitals_hour, by = c("ENCOUNTER_NUM", "hour")) 
```

## Processing vitals for all patients

Process:
```{r}
#| code-line-numbers: "|3-5|7-11|13-17|19-21"
all_processed_vitals_timeline <- all_vitals_timeline |>
  
  # LOCF
  tidyr::fill(contains('vital_')) |>
  select(-hour) |>
  
  # Group by encounter number and window,
  # summarize by mean
  group_by(ENCOUNTER_NUM, window_8hr) |>
  summarize_all(mean) |>
  ungroup() |>
  
  # Normalize the measurements
  tidyr::pivot_longer(cols = contains("vital_"), names_to = "variable") |>
  left_join(measurements_summary, by = "variable") |>
  mutate(filled_value = ifelse(is.na(value), mean, value)) |>
  mutate(normalized_value = (filled_value - q01) / (q99 - q01)) |>
  
  # Transform back to a wide dataframe
  select(ENCOUNTER_NUM, window_8hr, variable, normalized_value) |>
  tidyr::pivot_wider(names_from = variable, values_from = normalized_value)
```

## Processing vitals for all patients

```{r}
#| output-location: slide
all_processed_vitals_timeline |>
  head(50) |>
  datatable(options = list(pageLength=4))
```


## Creating the outcomes timeline {.fast}

We have a processed timeline of vitals. Let's get a a timeline of outcomes! Once we have that, we'll have all we need to train a model.

In order to create the outcomes timeline, we will use `encounters` and `all_timeline` (previously created).

```{r}
#| output-location: slide
encounters |>
  select(ENCOUNTER_NUM, OUTCOME_ALL, time_to_event) |>
  head() |>
  kable()
```

## Creating the outcomes timeline  {.fast}

```{r}
#| output-location: slide
all_timeline |>
  head() |>
  kable()
```

## Creating the outcomes timeline  {.fast}

Let's get the hour when the outcome happens:

```{r}
#| code-line-numbers: "|3|4|"
#| output-location: slide
outcome_hour <- encounters |>
  select(ENCOUNTER_NUM, OUTCOME_ALL, time_to_event) |>
  filter(OUTCOME_ALL == 1) |>
  mutate(outcome_hour = as.integer(ceiling(time_to_event))) |>
  select(ENCOUNTER_NUM, OUTCOME_ALL, outcome_hour)

outcome_hour |>
  head() |>
  kable()

```

## Creating the outcomes timeline 

Let's merge this back with the timeline and create the following columns:

- `outcome_24hrs`: the outcome occurs in the next 24 hours

- `outcome_48hrs`: the outcome occurs in the next 48 hours

- `outcome_72hrs`: the outcome occurs in the next 72 hours

## Creating the outcomes timeline  {.fast .scrollable}

```{r}
#| code-line-numbers: "|2|6-10"
#| output-location: slide
all_outcomes_timeline <- all_timeline |>
  left_join(outcome_hour, by = c("ENCOUNTER_NUM")) |>
  mutate(OUTCOME_ALL = as.integer(ifelse(is.na(OUTCOME_ALL), 0, OUTCOME_ALL))) |>
  group_by(ENCOUNTER_NUM) |>
  arrange(hour) |>
  mutate(
    outcome_24hrs = as.integer(ifelse(OUTCOME_ALL == 1 & outcome_hour - hour <= 24, 1, 0)),
    outcome_48hrs = as.integer(ifelse(OUTCOME_ALL == 1 & outcome_hour - hour <= 48, 1, 0)),
    outcome_72hrs = as.integer(ifelse(OUTCOME_ALL == 1 & outcome_hour - hour <= 24, 1, 0))
  ) |>
  ungroup() 

all_outcomes_timeline |> 
  filter(OUTCOME_ALL == 1) |>
  arrange(ENCOUNTER_NUM, hour) |>
  head(50) |>
  datatable(options = list(pageLength=4))
```

## Creating the outcomes timeline  {.fast}

Bin into 8-hour windows:

```{r}
#| code-line-numbers: "|2|6"
all_processed_outcomes_timeline <- all_outcomes_timeline |>
  group_by(ENCOUNTER_NUM, window_8hr) |>
  summarize(
    OUTCOME_ALL = max(OUTCOME_ALL),
    outcome_24hrs = max(outcome_24hrs),
    outcome_48hrs = max(outcome_48hrs),
    outcome_72hrs = max(outcome_72hrs)
  )  |>
  ungroup()
```

## Preventing label leakage  {.skip}

What is label leakage?

- Label leakage occurs when outcomes labels get "leaked" into model inputs.

## Preventing label leakage  {.skip}

Example from [Ghassemi et al., 2020](https://pmc.ncbi.nlm.nih.gov/articles/PMC7233077/):

> Consider predicting mortality of hospital patients using all available data up until their time of death. Such a task could lead to a pathological prediction rule—”if the ventilator is turned off in the preceding hour, predict death.” This commonly happens when patients and their families decide to withdraw care at a terminal stage of illness. A machine learning algorithm trained naively on this signal would have high predictive performance by nearly any metric, yet absolutely no clinical utility. 

## Preventing label leakage  {.skip}

How do we avoid this? Let's remove 6 hours before the outcome from the timeline

First, we address the hourly timeline:

```{r}
#| code-line-numbers: "|4-5|7|13-18|20"
all_outcomes_timeline <- all_timeline |>
  left_join(outcome_hour, by = c("ENCOUNTER_NUM")) |>
  mutate(OUTCOME_ALL = as.integer(ifelse(is.na(OUTCOME_ALL), 0, OUTCOME_ALL))) |>
  group_by(ENCOUNTER_NUM) |>
  arrange(hour) |>
  mutate(
    last_window = max(hour),
    outcome_24hrs = as.integer(ifelse(OUTCOME_ALL == 1 & outcome_hour - hour <= 24, 1, 0)),
    outcome_48hrs = as.integer(ifelse(OUTCOME_ALL == 1 & outcome_hour - hour <= 48, 1, 0)),
    outcome_72hrs = as.integer(ifelse(OUTCOME_ALL == 1 & outcome_hour - hour <= 24, 1, 0))
  ) |>
  ungroup() |>
  mutate(
    keep_row = case_when(
      OUTCOME_ALL == 0 ~ 1,
      OUTCOME_ALL == 1 & last_window - hour >= 6 ~ 1,
      TRUE ~ 0
    )
  ) |>
  filter(keep_row == 1) |>
  select(-keep_row)
```

## Preventing label leakage  {.skip}


Then, we can address the 8-hour timeline:

```{r}
#| code-line-numbers: "|2"
all_processed_outcomes_timeline <- all_outcomes_timeline |>
  group_by(ENCOUNTER_NUM, window_8hr) |>
  summarize(
    OUTCOME_ALL = max(OUTCOME_ALL),
    outcome_24hrs = max(outcome_24hrs),
    outcome_48hrs = max(outcome_48hrs),
    outcome_72hrs = max(outcome_72hrs)
  )  |>
  ungroup() 
```

## Model data

```{r}

model_df <- all_processed_outcomes_timeline |> 
  left_join(all_processed_vitals_timeline, by = c("ENCOUNTER_NUM", "window_8hr")) |>
  mutate(
    OUTCOME_ALL = as.factor(OUTCOME_ALL),
    outcome_24hrs = as.factor(outcome_24hrs),
    outcome_48hrs = as.factor(outcome_48hrs),
    outcome_72hrs = as.factor(outcome_72hrs)
  )
```


## Let's train a model!  {.fast .scrollable}

We'll use the `tidymodels` package. If you're interested in learning more, [this book is a great resource](https://www.tmwr.org/).

```{r}
library(tidymodels)
```

We split the data into an 80/20 split.

The `group` variable ensures all observations with the same group value (i.e., `ENCOUNTER_NUM`) end up in the same split.

```{r}
#| code-line-numbers: "|5|4"
train_test_split <- group_initial_split(
  model_df, 
  prop = 0.8, 
  strata = "OUTCOME_ALL", 
  group = "ENCOUNTER_NUM"
)
training_df <- training(train_test_split)
testing_df <- testing(train_test_split)
```

## Let's train a model!  {.fast}

We are going to train a model to predict `outcome_48hrs` based on vitals.

```{r}
model_formula <- as.formula(outcome_48hrs ~ vital_sbpdiastolic + vital_sbpsystolic + vital_spulse + vital_srespirations + vital_stemperature + vital_sfio2)
```

## Let's train a model!  {.fast}

We use the `glmnet` package to train a logistic regression model:

- The `penalty` parameter specifies the amount of regularization.

- The `mixture` parameter specifies the type of regularization, i.e., the proportion of lasso penalty (1 = lasso model, 0 = ridge regression, anywhere between 0 and 1 is an elastic net model)

```{r}
#| code-line-numbers: "|2"
lr_mod <- 
  logistic_reg(penalty = tune(), mixture = tune()) |> 
  set_engine("glmnet")

```
 
## Let's train a model!  {.fast}

```{r}
lr_recipe <- recipe(model_formula, data = training_df)
lr_workflow <- workflow() |> 
  add_model(lr_mod) |>
  add_recipe(lr_recipe)
```

Let's tune hyperparameters!

## Let's train a model!  {.fast}

One way to do this is through **grid search**, but this might be long...

```{r}
cv_folds <- vfold_cv(training_df, strata = outcome_48hrs, v = 5)
```

```{r, eval = FALSE}
#| code-line-numbers: "|1-3|5-10"
# Grid search
lr_reg_grid <- grid_regular(penalty(range = c(-4, -2)), levels = 100) |>
  expand_grid(mixture = c(0, 0.25, 0.5, 0.75, 1))

lr_res <-
  lr_workflow |>
  tune_grid(cv_folds,
            grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))

lr_res |>
  collect_metrics() |>
  ggplot(aes(x = penalty, y = mean)) +
  facet_wrap(. ~ mixture) +
  geom_point() +
  geom_line() +
  ylab("Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number()) +
  theme_minimal()

```

## Let's train a model!  {.fast}

An alternative way to tune this is through Bayes tuning:

```{r}
#| code-line-numbers: "|3|4-5"
# Bayes tuning
lr_res <- lr_workflow |>
  tune_bayes(cv_folds,
             iter = 100,
             initial = 5,
             control = control_bayes(save_pred = TRUE),
             metrics = metric_set(roc_auc))

# Ended up running for x iterations only
lr_res |>
    nrow()
```

## Let's train a model!  {.fast}

```{r}
#| output-location: slide
lr_res |>
  collect_metrics() |>
  ggplot(aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  ylab("Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number()) +
  theme_minimal()
```

## Let's train a model!  {.fast}

```{r}
#| output-location: slide
lr_res |>
  collect_metrics() |>
  ggplot(aes(x = mixture, y = mean)) +
  geom_point() +
  geom_line() +
  ylab("Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number()) + 
  theme_minimal()
```

## Let's train a model!  {.fast}

After finding the best hyper-parameters, we can then re-fit the model on the whole training data:

```{r}
#| code-line-numbers: "|1-2"
lr_best <- lr_res |>
  select_best()

lr_best |>
  kable()
```

## Let's train a model!  {.fast}

After finding the best hyper-parameters, we can then re-fit the model on the whole training data:

```{r}
#| code-line-numbers: "|1-3|5-6"
final_wf <-
  lr_workflow |>
  finalize_workflow(lr_best)

final_fit <- final_wf |>
  fit(training_df)
```

## Let's train a model!  {.fast}

Let's look at results:

```{r}
train_predictions <- final_fit |>
    predict(training_df, type = "prob") |>
    cbind(training_df)
roc_auc(train_predictions,
        truth = outcome_24hrs,
        .pred_1,
        event_level = "second") |>
  kable()
```

## Let's train a model!  {.fast}

Let's look at results:

```{r}
test_predictions <- final_fit |>
    predict(testing_df, type = "prob") |>
    cbind(testing_df)
roc_auc(test_predictions,
        truth = outcome_24hrs,
        .pred_1,
        event_level = "second") |>
  kable()
```


## Let's train a model!  {.fast}

```{r}
final_fit |>
  predict(testing_df, type = "prob") |>
  cbind(testing_df) |>
  ggplot(aes(x = .pred_1, fill = outcome_48hrs)) +
  geom_boxplot() +
  theme_minimal() +
  theme(legend.position = "top")
```

## XGBoost {.skip}

Not great!

We can try out XGBoost!

```{r}
xgboost_mod <-
    boost_tree(trees = tune(),
                      min_n = tune(),
                      tree_depth = tune(),
                      learn_rate = tune(),
                      loss_reduction = tune(),
                      mtry = tune(), sample_size = tune()) |>
  set_mode("classification") |>
  set_engine("xgboost")
```

## XGBoost {.skip}

[Tuning parameters](https://parsnip.tidymodels.org/reference/details_boost_tree_xgboost.html)

```{r}
xgboost_recipe <- recipe(model_formula, data = training_df)
xgboost_workflow <- workflow() |>
  add_model(xgboost_mod) |>
  add_recipe(xgboost_recipe)
```

## XGBoost {.skip}

[Tuning parameters](https://parsnip.tidymodels.org/reference/details_boost_tree_xgboost.html)

```{r}
xgboost_params <-
  tune::parameters(
    dials::trees(),
    dials::min_n(),
    dials::tree_depth(),
    dials::learn_rate(),
    dials::loss_reduction(),
    sample_size = dials::sample_prop(),
    mtry = dials::finalize(dials::mtry(), training_df)
  )
```

## XGBoost {.skip}

Instead of grid search, let's tune with Bayes:

```{r}

xgboost_res <- xgboost_workflow |>
  tune_bayes(cv_folds,
             iter = 100,
             initial = 5,
             param_info = xgboost_params,
             control = control_bayes(save_pred = FALSE, verbose_iter = TRUE, verbose = TRUE),
             metrics = metric_set(roc_auc))
```

## XGBoost {.skip}

Once we have the best hyperparameters, we can re-fit the model with the training data.

```{r}
xgboost_best <- xgboost_res |>
  select_best()

xgboost_best |>
  kable()
```

## XGBoost {.skip}

Once we have the best hyperparameters, we can re-fit the model with the training data.

```{r}
xgboost_final_wf <-
  xgboost_workflow |>
  finalize_workflow(xgboost_best)

xgboost_final_fit <- xgboost_final_wf |>
  fit(training_df)
```

## XGBoost {.skip}

Let's look at results!

```{r}
xgboost_train_predictions <- xgboost_final_fit |>
    predict(training_df, type = "prob") |>
    cbind(training_df)

roc_auc(xgboost_train_predictions,
        truth = outcome_24hrs,
        .pred_1,
        event_level = "second") |>
  kable()
```

## XGBoost {.skip}
 
Let's look at results!

```{r}
xgboost_test_predictions <- xgboost_final_fit |>
    predict(testing_df, type = "prob") |>
    cbind(testing_df)
roc_auc(xgboost_test_predictions,
        truth = outcome_24hrs,
        .pred_1,
        event_level = "second") |>
  kable()
```

